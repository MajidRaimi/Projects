{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e6ba57",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-29T05:41:55.034683Z",
     "iopub.status.busy": "2022-06-29T05:41:55.034169Z",
     "iopub.status.idle": "2022-06-29T05:41:55.054117Z",
     "shell.execute_reply": "2022-06-29T05:41:55.052401Z"
    },
    "papermill": {
     "duration": 0.028721,
     "end_time": "2022-06-29T05:41:55.057094",
     "exception": false,
     "start_time": "2022-06-29T05:41:55.028373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/sample-submission.csv\n",
      "input/test.csv\n",
      "input/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7b9496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:41:55.068728Z",
     "iopub.status.busy": "2022-06-29T05:41:55.068269Z",
     "iopub.status.idle": "2022-06-29T05:42:07.698659Z",
     "shell.execute_reply": "2022-06-29T05:42:07.697494Z"
    },
    "papermill": {
     "duration": 12.639312,
     "end_time": "2022-06-29T05:42:07.701503",
     "exception": false,
     "start_time": "2022-06-29T05:41:55.062191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37cf567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:42:07.712748Z",
     "iopub.status.busy": "2022-06-29T05:42:07.711984Z",
     "iopub.status.idle": "2022-06-29T05:42:09.750830Z",
     "shell.execute_reply": "2022-06-29T05:42:09.749623Z"
    },
    "papermill": {
     "duration": 2.047293,
     "end_time": "2022-06-29T05:42:09.753474",
     "exception": false,
     "start_time": "2022-06-29T05:42:07.706181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"input/\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / \"train.csv\", dtype=\"uint8\")\n",
    "test_df = pd.read_csv(DATA_DIR / \"test.csv\", dtype=\"uint8\")\n",
    "\n",
    "\n",
    "train_features = train_df.drop(\"labels\", axis=1)\n",
    "train_target = train_df.loc[:, \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dcdcc72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:42:09.764620Z",
     "iopub.status.busy": "2022-06-29T05:42:09.764267Z",
     "iopub.status.idle": "2022-06-29T05:42:09.775004Z",
     "shell.execute_reply": "2022-06-29T05:42:09.773882Z"
    },
    "papermill": {
     "duration": 0.019258,
     "end_time": "2022-06-29T05:42:09.777460",
     "exception": false,
     "start_time": "2022-06-29T05:42:09.758202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSetWithTransforms(Dataset):\n",
    "    \n",
    "    def __init__(self, features, target, feature_transforms=None):\n",
    "        super().__init__()\n",
    "        self._features = features\n",
    "        self._target = torch.from_numpy(target).long()\n",
    "        self._feature_transforms = feature_transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self._feature_transforms is None:\n",
    "            features = self._features[index]\n",
    "            #feature = torch.rashape(feature, (32,32,3))\n",
    "        else: \n",
    "            features = self._feature_transforms(self._features[index])\n",
    "        target = self._target[index]\n",
    "        return (features, target) \n",
    "    \n",
    "    def __len__(self):\n",
    "        n_samples, _ = self._features.shape\n",
    "        return n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52517dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:42:09.788346Z",
     "iopub.status.busy": "2022-06-29T05:42:09.787967Z",
     "iopub.status.idle": "2022-06-29T05:42:09.797615Z",
     "shell.execute_reply": "2022-06-29T05:42:09.796849Z"
    },
    "papermill": {
     "duration": 0.017615,
     "end_time": "2022-06-29T05:42:09.799740",
     "exception": false,
     "start_time": "2022-06-29T05:42:09.782125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, features, feature_transforms=None):\n",
    "        super().__init__()\n",
    "        self._features = features\n",
    "        self._feature_transforms = feature_transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self._feature_transforms is None:\n",
    "            features = self._features[index]\n",
    "            #feature = torch.rashape(feature, (32,32,3))\n",
    "        else: \n",
    "            features = self._feature_transforms(self._features[index])\n",
    "        return (features) \n",
    "    \n",
    "    def __len__(self):\n",
    "        n_samples, _ = self._features.shape\n",
    "        return n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6697d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:42:09.810569Z",
     "iopub.status.busy": "2022-06-29T05:42:09.810189Z",
     "iopub.status.idle": "2022-06-29T05:42:09.818580Z",
     "shell.execute_reply": "2022-06-29T05:42:09.816982Z"
    },
    "papermill": {
     "duration": 0.017051,
     "end_time": "2022-06-29T05:42:09.821486",
     "exception": false,
     "start_time": "2022-06-29T05:42:09.804435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data augmentation should only apply to training data\n",
    "_feature_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda array: array.reshape((32, 32))),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=15, scale=(1.0, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffa1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conveting test data only to tensor images should only apply to training data\n",
    "_feature_transforms_test = transforms.Compose([\n",
    "    transforms.Lambda(lambda array: array.reshape((32, 32))),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6a237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "def mnist(batch_sz, valid_size=0.2, shuffle=True, random_seed=2000):\n",
    "    \n",
    "\n",
    "    train_df = pd.read_csv(DATA_DIR / \"train.csv\", dtype=\"uint8\")\n",
    "    test_df = pd.read_csv(DATA_DIR / \"test.csv\", dtype=\"uint8\")\n",
    "\n",
    "    train_features = train_df.drop(\"labels\", axis=1)\n",
    "    train_target = train_df.loc[:, \"labels\"]\n",
    "\n",
    "    # Training dataset\n",
    "    train_data = DataSetWithTransforms(train_features.values, train_target.values, _feature_transforms)\n",
    "    valid_data = DataSetWithTransforms(train_features.values, train_target.values, _feature_transforms_test)\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    if shuffle == True:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_sz, sampler=train_sampler, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=batch_sz, sampler=valid_sampler, pin_memory=True)\n",
    "\n",
    "    # Test dataset\n",
    "    test_data = DataSetTest(test_df.values, _feature_transforms_test)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0f62536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T05:42:09.832382Z",
     "iopub.status.busy": "2022-06-29T05:42:09.832004Z",
     "iopub.status.idle": "2022-06-29T05:42:09.837793Z",
     "shell.execute_reply": "2022-06-29T05:42:09.836898Z"
    },
    "papermill": {
     "duration": 0.013628,
     "end_time": "2022-06-29T05:42:09.839745",
     "exception": false,
     "start_time": "2022-06-29T05:42:09.826117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_sz=64 # this is batch size i.e. the number of rows in a batch of data\n",
    "train_loader, valid_loader, test_loader=mnist(batch_sz) \n",
    "for batch in test_loader:\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfe5fc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13440"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*(len(train_loader)+len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "780d27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1)\n",
    "        self.norm1 = nn.BatchNorm2d(20)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.norm2 = nn.BatchNorm2d(50)\n",
    "        self.l1 = nn.Linear(5*5*50,100)\n",
    "        self.l2 = nn.Linear(100,200)\n",
    "        self.l3 = nn.Linear(200, 500)\n",
    "        self.l4 = nn.Linear(500, 200)\n",
    "        self.l5 = nn.Linear(200, 29)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (32,32)\n",
    "        x = F.leaky_relu(self.norm1(self.conv1(x)))\n",
    "        # (28,28)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # (14,14)\n",
    "        x = F.leaky_relu(self.norm2(self.conv2(x)))\n",
    "        # (10,10)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # (5,5)\n",
    "        x = x.view(-1,5*5*50)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.l4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.l5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ee47ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "net = ConvNet()\n",
    "net = net.to(device)\n",
    "\n",
    "ls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "24786929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "net = models.resnet18(num_classes=29)\n",
    "\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "af563517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=(5, 5), padding=(3, 3), bias=False)\n",
    "#net.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
    "net = net.to(device)\n",
    "lr = 0.1\n",
    "momentum = 0.2\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=lr, momentum=momentum)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b056eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "826a39e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825148809523809\n"
     ]
    }
   ],
   "source": [
    "print(highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "  loss_total=0\n",
    "  loss_val=0\n",
    "  acc_train=0\n",
    "  total_train=0\n",
    "  net.train()\n",
    "  for ii,batch in enumerate(train_loader):\n",
    "    data=batch[0]\n",
    "    label=batch[1]\n",
    "    #optimizer-->buffer += grad\n",
    "    optimizer.zero_grad()\n",
    "    data, label = data.to(device), label.to(device)\n",
    "    logits = net(data)\n",
    "    #print(type(logits))\n",
    "    #this is the output of the network and it's shape is batch_size X no of classes\n",
    "    loss = F.cross_entropy(logits, label)\n",
    "    loss_total+=loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    out=torch.argmax(logits, dim=1)\n",
    "    acc_train+=torch.sum(out==label)\n",
    "    total_train+=logits.shape[0]\n",
    "\n",
    "  acc_val=0\n",
    "  total_val=0 \n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "    for jj,batch in enumerate(valid_loader):\n",
    "          data=batch[0]\n",
    "          label=batch[1]\n",
    "          #optimizer-->buffer += grad\n",
    "          data, label = data.to(device), label.to(device)\n",
    "          logits = net(data)\n",
    "          loss = F.cross_entropy(logits, label)\n",
    "          loss_val+=loss.item()\n",
    "          out=torch.argmax(logits, dim=1)\n",
    "          acc_val+=torch.sum(out==label)\n",
    "          total_val+=logits.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "  ls.append(loss_total)\n",
    "  lr_scheduler.step()\n",
    "  print(f\"Iterataion {i}: Training Loss: {loss_total/ii}, Validation Loss: {loss_val/jj}\")\n",
    "  print(f\"Iteataion {i}: Training Accuracy: {acc_train.item()/total_train}, Validation Accuracy  {acc_val.item()/total_val}\")\n",
    "  if ((acc_val.item()/total_val) > highest):\n",
    "    highest = acc_val.item()/total_val\n",
    "    break\n",
    "    \n",
    "plt.plot(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "303fa597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9840029761904762\n"
     ]
    }
   ],
   "source": [
    "acc_val=0\n",
    "total_val=0 \n",
    "with torch.no_grad():\n",
    "      for jj,batch in enumerate(valid_loader):\n",
    "            data=batch[0]\n",
    "            label=batch[1]\n",
    "            #optimizer-->buffer += grad\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            logits = net(data)\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            loss_val+=loss.item()\n",
    "            out=torch.argmax(logits, dim=1)\n",
    "            acc_val+=torch.sum(out==label)\n",
    "            total_val+=logits.shape[0]\n",
    "\n",
    "print(f\"Testing Accuracy: {acc_val.item()/total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8458cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = list()\n",
    "for images in test_loader:\n",
    "    prediction_list.append(net(images.cuda()).argmax(1))\n",
    "    \n",
    "prediction_list = torch.cat(prediction_list)\n",
    "test_features = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "_ = (pd.DataFrame\n",
    "       .from_dict({\"Id\": test_features.index, \"Category\": prediction_list.cpu()})\n",
    "       .to_csv(\"submission.csv\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277d6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.93097,
   "end_time": "2022-06-29T05:42:18.383685",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-29T05:41:43.452715",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0d9c7fd3642b660ffedf3a77d11c31e11ccbd2a913cb9ccb12c86252838bb24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
